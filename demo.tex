% !TEX root = ./rep.tex
\section{Demonstration Simulations}
\label{s:demo}

We demonstrate here the use of Cardinal on two pebble-bed configurations
that are representative of FHR cores.  The first case consists of 146
pebbles and the second uses 1568 pebbles.

\subsection{146 Pebbles}
\label{ss:c4}

Using the Nek5000 model of the TAMU experiment as a basis, we have developed a
multiphysics simulation of a bed comprising 146 pebbles.  The pebble model for
Nek5000 reflects the practices used for the TAMU experiment, which were
validated carefully against experimental PIV data. Inlet/outlet boundary
conditions are used (Figure~\ref{f:pb2}). Unlike the TAMU experiment, here the
mesh is designed to allow clearance between pebbles. This design facilitates
the coupling, but it will likely be updated in future simulations. The
difference is outlined in Figure~\ref{f:pb2}. The mesh was constructed
using the tet-to-hex approach and comprises approximately 500,000 elements
overall. It is designed to run at $N=5$ for coarse results and $N=7$ for
finer simulations (for a max of 256 million grid points). We
assume constant properties.

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.8\textwidth]{Figures/pb_mesh}
\caption{Mesh and boundary conditions for Nek5000 problem.}
\label{f:pb2}
\end{figure}

To test various options and accelerate the development, we defined four
variants of the demo problem, all available in the Cardinal repository. The
variants reflect the need to define cheaper transients for testing purposes.
Table~\ref{tab:nek} shows the cases: they are listed in order of increasing
computational cost. Restarting from an advanced restart leads to faster
convergence. Moreover, simulating the full Navier-Stokes is considerably more
expensive than assuming a ``frozen velocity'' and solving only the
advection-diffusion equation.

\begin{table}
  \centering
  \begin{tabular}{|lcc|}
    \hline \hline
    Case & Restart Condition & Solver \\
    \hline
    1 & Advanced restart state & Advection-Diffusion only \\
    2 & Constant temperature   & Advection-Diffusion only \\
    3 & Advanced restart state & Full Navier-Stokes \\
    4 & Constant temperature   & Full Navier-Stokes \\
    \hline \hline
  \end{tabular}
  \caption{Nek5000 Cases with Various Volver Options}
  \label{tab:nek}
\end{table}

The OpenMC model is consistent with the pebble-bed model discussed in the validation section. For BISON and the demonstration problem, we consider only the conduction equation, and hence it is a relatively straightforward setup. Properties are constant and adapted from available correlations. The mesh for a single sphere is generated and replicated at runtime. Only cell-based tallies are considered for this demonstration.

For the coupled simulations, we opted to tightly couple Nek5000 and BISON and loosely couple OpenMC, given that if OpenMC is executed at every step, the computational cost becomes excessive. We performed the OpenMC solve every 100 steps. The simulations were performed on 2,000 processors. Snapshots of the state after an initial transient are shown in Figure~\ref{f:dtamu1} and Figure~\ref{f:dtamu2}. In particular, we observe the following:
\begin{itemize}
  \item The effect of the complex flow field on the surface temperature of the pebbles.
  \item A slight tilt in the temperature distribution in the fuel's interior due to the outer temperature condition of each pebble.
  \item The effect of the reflective boundary conditions on the power distribution and a slight power tilt toward the bottom due to the colder coolant.
\end{itemize}

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.9\textwidth]{Figures/demo_r1}
\caption{TAMU demo results. From left to right: snapshots of temperature on surface, average temperature in solid and average heating.}
\label{f:dtamu1}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.9\textwidth]{Figures/demo_r2}
\caption{TAMU Demo Results. Right -- temperature in the solid. Left -- temperature details in the fluid.}
\label{f:dtamu2}
\end{figure}

\subsection{1568 Pebbles}

\subsubsection{Numerical Setup}

We also present a case comprising 1568 pebbles. The pebble configuration was
obtained by using a discrete element method (DEM)
code~\cite{projectChronoWebSite}.  The mesh has $\approx$ 524,000 elements and
was generated using the Voronoi-cell strategy described in Section 2.2.2.
The configuration is illustrated in Figure~\ref{f:ndemo1}.
Meshes for higher pebble counts exhibit similar element-to-pebble ratios.
We note that 1568 pebbles is a significant size for a coupled calculation and
representative of the SANA experiments \cite{zou2017validation}, for instance.

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.9\textwidth]{Figures/ndemo_r1}
\caption{NekRS: A spectral element mesh using 524,386 spectral elements for 1568 pebble configuration.
         The mesh is generated by an all-hex meshing tool based on Voronoi cell strategy.}
\label{f:ndemo1}
\end{figure}

For this demonstration, the sizes and composition of the TRISO particles were
based on TRISO manufactured at INL, following the practice established in the
validation section. Although these particles were developed for the AGR fuel,
particles with the same specifications are used for FHR test reactors and
computation benchmarks. The sizes and compositions of the pebbles were taken
from the Mark-1 FHR reactor constructed at UC Berkeley. For BISON and the
demonstration problem, we consider only the conduction equation, and hence  it
is a relatively straightforward setup. Properties are constant and adapted from
available correlations. The mesh for a single sphere is generated and
replicated at runtime. The same mesh is used for the mesh tallies in OpenMC.

\subsubsection{Results}
We begin by demonstrating strong-scaling performance of NekRS (GPU) and NekRS
(CPU) in Table~\ref{tab:nekrs}.  We measured timings for 100 timesteps for
turbulent flow simulations with the 1568-pebble mesh on ORNL's Summit, using 42
MPI ranks per node for the CPU runs and 6 Nvidia V100s per node for the GPU
runs.  For the same node count, the GPU-accelerated variant of NekRS is more
than 9$\times$ faster when using 3.5 million points per GPU (here, 7283
spectral elements per GPU and $N=7$).  The NekRS GPU run realizes 82\%
parallel efficiency at 2.1 million points per GPU.


\begin{table}
  \centering
  \begin{tabular}{c|cccc||cccc}
    \hline 
 % \multirow{2}{*}{ } &
  \multicolumn{1}{c|}{ } &
  \multicolumn{4}{|c||}{NekRS GPU}  &
  \multicolumn{4}{|c}{NekRS CPU} \\
  \hline
  %\cline{2-5}
    Nodes  &  $E$/GPU & $n$/GPU & walltime/step (s) & eff & $E$/MPI& $n$/MPI & walltime/step (s) & eff\\
  \hline
    12 & 7283 & 2.4M & 0.569 & 1.00 & 1040 & 356K & 5.361  & 1.00\\
    20 & 4369 & 1.4M & 0.416 & 0.82 & 624 & 214K  & 3.055  & 1.05\\
    28 & 3121 & 1.0M & 0.338 & 0.72 & 445 & 152K & 2.200  & 1.04\\
    36 & 2427 & 0.8M & 0.309 & 0.61 & 346 & 118K & 1.876  & 0.95\\
    44 & 1986 & 0.6M & 0.290 & 0.53 & 283 & 97K & 1.401  & 1.04\\
    52 & 1680 & 0.5M & 0.255 & 0.51 & 240 & 82K & 1.204  & 1.02\\
    \hline \hline
  \end{tabular}
  \caption{NekRS GPU/CPU Strong-Scale Timings (seconds per step) for 100 Steps of Turbulent flow Simulations
   with $Re=10000$ for 1568-Pebble Case Using Total Number of Grid Points $n=179,864,398$ ($E=524,386$ and $N=7$).} 
  \label{tab:nekrs}
\end{table}

For the coupled-physics demonstration, the 1568 model described in the preceding
section has been run on up to 20 nodes of Summit, with 6 MPI ranks on each
node, corresponding to the 6 GPUs on each node. The OpenMC and BISON models are
designed to run on CPUs, while the NekRS model runs on the GPU. Prior to
coupling, NekRS was used to run a stand-alone LES out to 25 convective time
units in order develop generate a turbulent initial condition, which is shown
in Figure~\ref{f:ndemo3}.

The flow initial condition was used to restart a coupled transient simulation in
Cardinal representing the pebbles' heat-up. The timestep was fixed to $5\times
10^{-4}$ s in both BISON and NekRS. The temperature at time zero was set to 300
$^{\circ}$C everywhere. Figure~\ref{f:ndemo4} presents the temperature at the
surface of the pebbles in BISON at three points in time. The simulations took
2.5 s per coupled time step on 20 nodes, requiring transfer between physics at
each timestep. However, this time could be significantly optimized by relaxing
the requirement, since data transfer from GPU to CPU should be minimized as
much as possible.

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.9\textwidth]{Figures/ndemo_r3}
\caption{NekRS: a velocity component for turbulent flow simulations using the
spectral element mesh with 524,386 spectral elements for 1568 pebbles from the
all-hex meshing tool based on Voronoi cell strategy. } 
\label{f:ndemo3}
\end{figure}


\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.9\textwidth]{Figures/ndemo_r4}
\caption{Temperature result for the pebble surface temperature at three points in time.}
\label{f:ndemo4}
\end{figure}

The results of OpenMC simulations coupled with the heat conduction module are
shown in Figures~\ref{f:1568_openmc_heat_source} and
~\ref{f:1568_openmc_temperatures} with the same time step parameters as the
NekRS simulations described above. An eigenvalue simulation using 150 batches
with 50 inactive batches was executed in each time step. 50,000 particles per
batch were used to converge the pebble-averaged heat source from the OpenMC cell
tally while the unstructured mesh heat source tally required 500,000 particles
per batch to produce the heat source presented here. Production simulations may
require an even higher number of particles per batch to more tightly converge
the heating distribution when using the unstructured mesh heat source because of the
decreased number of samples per source particle in the tally bins.
Figure~\ref{f:1568_openmc_temperatures_single_pebble} demonstrates the effect of
the improved spatial resolution provided by the unstructured mesh heat source
from OpenMC on the temperature distribution within a representative pebble. In
the case of the pebble-averaged heat source the temperature profile is
symmetric, reflecting the uniform heat source applied in that region, while an
asymmetry can be seen in the profile generated from the unstructured mesh heat
source, indicating that the improved spatial resolution of the source has an
impact on the temperature distribution in the solid and will in turn affect the
resulting heat transfer to the fluid.

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.48\textwidth]{Figures/openmc_cell_heat_source}
\includegraphics[clip=true,width=0.48\textwidth]{Figures/openmc_mesh_heat_source}
\caption{Left: Heat source using the original cell tallies to produces an average heat source per-pebble. Right: Heat source produced using an OpenMC unstructured mesh tally.}
\label{f:1568_openmc_heat_source}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.48\textwidth]{Figures/openmc_cell_temperature}
\includegraphics[clip=true,width=0.48\textwidth]{Figures/openmc_mesh_temperature}
\caption{Left: Temperature in the solid resulting from the cell-based heating tally. Right: Temperature in the solid resulting from the unstructured mesh heating tally in OpenMC.}
\label{f:1568_openmc_temperatures}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[clip=true,width=0.48\textwidth]{Figures/openmc_cell_temperature_zoomed}
\includegraphics[clip=true,width=0.48\textwidth]{Figures/openmc_mesh_temperature_zoomed}
\caption{Temperature profiles of the same pebble in the 1568 pebble demo using
the pebble-averaged heating (left) and the unstructured mesh heating (right).}
\label{f:1568_openmc_temperatures_single_pebble}
\end{figure}

\subsection{Projection to Full Core}

Twenty nodes represents less than 1\% of the computing power available on
Summit, which is sufficient for 1568 pebbles at 2.1 million points per
GPU.  Performance throughput for NekRS saturates at 2--4 million points
per GPU on Summit.  By running with 4 million points per GPU, we estimate that
80\% of the machine will be sufficient to perform a full-core calculation in
FHRs corresponding to 300,000 pebbles.

%% NOTE: can remove table data if not necessary
% Table~\ref{tab:pebble} demonstrates our current all-hex meshing capability
% based on Voronoi cell strategy up to 11,145 pebbles using 3.5 million spectral
% elements (total 2 million grid points, $n = EN^3$) 
% and their averaged walltime per timestep from flow simulations with
% $Re= 5000$ up to time $=7$. % involving 14000--15750 time steps.  
% \begin{table}
%   \centering
%   \begin{tabular}{cccccccc}
%   \hline \hline
%     \# pebbles & $E$ & $E$/pebble & \# nodes & \# GPUs & $E$/GPU & $n$/GPU & walltime/step (s) \\
%   \hline
%    146 & 62,132 & 425 & 2 & 12 & 5177 & 2.6M &  0.34    \\
%    1568 & 524,386 & 334 & 28 & 168 & 3121 & 1.5M & 0.40   \\
%    3260 & 1,121,214 & 343 & 50 & 300 & 3737 & 1.9M & 0.75   \\
%    11145 & 3,575,076 & 320 & 148 & 888 & 4025 & 2.0M & 0.90  \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %  \# pebbles & $E$ & $E$/pebble & \# nodes & \# GPU & $E$/GPU & $n$/GPU & timesteps & walltime ($T=7$) \\
% %  146 & 62,132 & 425 & 2 & 12 & 5177 & 2.6M & 14,000 timesteps (dt=5e-4) & 4860.87 \\
% %  1568 & 524,386 & 334 & 28 & 168 & 3121 & 1.5M & 14,000   (dt=5e-4)    & 5735.58  \\
% %  3260 & 1,121,214 & 343 & 50 & 300 & 3737 & 1.9M & 15,750  (dt= 5e-4 or dt=4e-4)   & 11855.54  \\  %% <--- 15750= 7k (5e-4) + 8750 (4e-4) 
% %  11145 & 3,575,076 & 320 & 148 & 888 & 4025 & 2.0M & 14,000  (dt=5e-4)  & 12639.43  \\
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     \hline \hline
%   \end{tabular}
%   \caption{NekRS: All-Hex Meshes for Pebbles, Based on Voronoi Cell strategy, and Simulation Timings (seconds per step) on GPUs
%    of Turbulent Flow Simulations with $Re = 5000$.}
%   \label{tab:pebble}
% \end{table}
% 
